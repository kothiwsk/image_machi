{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 1\n",
    "------------\n",
    "\n",
    "The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.\n",
    "\n",
    "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset to be used with python experiments. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from PIL import Image\n",
    "from sklearn import cross_validation\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "train_filename = 'train'\n",
    "test_filename = 'test'\n",
    "\n",
    "image_size_w = 128  # Pixel width and height.\n",
    "image_size_h = 78\n",
    "size = 128, 128\n",
    "pixel_depth = 255.0  # Number of levels per pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNWGtZaXn-5j"
   },
   "source": [
    "First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given these sizes, it should be possible to train models quickly on any machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC3p0oEyF8QT"
   },
   "source": [
    "Extract the dataset from the compressed .tar.gz file.\n",
    "This should give you a set of directories, labelled A through J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 186055,
     "status": "ok",
     "timestamp": 1444485672525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "H8CBE-WZ8nmj",
    "outputId": "ef6c790c-2513-4b09-962e-27c79390c762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already present - Skipping extraction of train.\n",
      "['train\\\\ALB', 'train\\\\BET', 'train\\\\DOL', 'train\\\\LAG', 'train\\\\NoF', 'train\\\\OTHER', 'train\\\\SHARK', 'train\\\\YFT']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 8\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = ['test\\\\test_stg1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train\\\\ALB', 'train\\\\BET', 'train\\\\DOL', 'train\\\\LAG', 'train\\\\NoF', 'train\\\\OTHER', 'train\\\\SHARK', 'train\\\\YFT']\n",
      "Pickling train\\ALB.pickle.\n",
      "1719\n",
      "train\\ALB\n",
      "Full dataset tensor: (1719, 78, 128)\n",
      "Mean: -0.136433\n",
      "Standard deviation: 0.230907\n",
      "(1719, 78, 128)\n",
      "Pickling train\\BET.pickle.\n",
      "200\n",
      "train\\BET\n",
      "Full dataset tensor: (200, 78, 128)\n",
      "Mean: -0.129147\n",
      "Standard deviation: 0.233692\n",
      "(200, 78, 128)\n",
      "Pickling train\\DOL.pickle.\n",
      "117\n",
      "train\\DOL\n",
      "Full dataset tensor: (117, 78, 128)\n",
      "Mean: -0.143655\n",
      "Standard deviation: 0.243055\n",
      "(117, 78, 128)\n",
      "Pickling train\\LAG.pickle.\n",
      "67\n",
      "train\\LAG\n",
      "Full dataset tensor: (67, 78, 128)\n",
      "Mean: -0.123839\n",
      "Standard deviation: 0.222084\n",
      "(67, 78, 128)\n",
      "Pickling train\\NoF.pickle.\n",
      "465\n",
      "train\\NoF\n",
      "Full dataset tensor: (465, 78, 128)\n",
      "Mean: -0.126385\n",
      "Standard deviation: 0.226744\n",
      "(465, 78, 128)\n",
      "Pickling train\\OTHER.pickle.\n",
      "299\n",
      "train\\OTHER\n",
      "Full dataset tensor: (299, 78, 128)\n",
      "Mean: -0.148818\n",
      "Standard deviation: 0.215283\n",
      "(299, 78, 128)\n",
      "Pickling train\\SHARK.pickle.\n",
      "176\n",
      "train\\SHARK\n",
      "Full dataset tensor: (176, 78, 128)\n",
      "Mean: -0.118036\n",
      "Standard deviation: 0.223568\n",
      "(176, 78, 128)\n",
      "Pickling train\\YFT.pickle.\n",
      "734\n",
      "train\\YFT\n",
      "Full dataset tensor: (734, 78, 128)\n",
      "Mean: -0.134337\n",
      "Standard deviation: 0.239031\n",
      "(734, 78, 128)\n",
      "['test\\\\test_stg1']\n",
      "Pickling test\\test_stg1.pickle.\n",
      "1000\n",
      "test\\test_stg1\n",
      "Full dataset tensor: (1000, 78, 128)\n",
      "Mean: -0.1389\n",
      "Standard deviation: 0.228672\n",
      "(1000, 78, 128)\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  filelist = glob.glob(folder+\"\\*.thumbnail\")\n",
    "  for f in filelist:\n",
    "    os.remove(f)\n",
    "    \n",
    "    \n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  print(len(image_files))\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size_h, image_size_w),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  file_names=[]\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    outfile = image_file + \".thumbnail\"\n",
    "    im = Image.open(image_file)\n",
    "    \n",
    "    horizontal_padding = (1300 - im.size[0]) / 2\n",
    "    vertical_padding = (800 - im.size[1]) / 2\n",
    "    img1 = im.crop(\n",
    "        (\n",
    "            -horizontal_padding,\n",
    "            -vertical_padding,\n",
    "            im.size[0] + horizontal_padding,\n",
    "            im.size[1] + vertical_padding\n",
    "        )\n",
    "    )\n",
    "    img1.thumbnail(size, Image.ANTIALIAS)\n",
    "    img1.save(outfile, \"JPEG\")\n",
    "    try:\n",
    "      image_data = (ndimage.imread(outfile,flatten=True).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size_h, image_size_w):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    file_names.append(image)\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  print(dataset.shape)\n",
    "  return dataset, file_names\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  image_names = []\n",
    "  print(data_folders)\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset, file_names = load_letter(folder, min_num_images_per_class)\n",
    "      image_names.extend(file_names)         \n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names, image_names\n",
    "\n",
    "train_datasets, train_images = maybe_pickle(train_folders, 67)\n",
    "test_datasets, test_images = maybe_pickle(test_folders, 353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test\\\\test_stg1.pickle']\n"
     ]
    }
   ],
   "source": [
    "print(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1719, 78, 128)\n",
      "(200, 78, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((1719, 1), 0) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((200, 1), 1) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((117, 1), 2) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 78, 128)\n",
      "(67, 78, 128)\n",
      "(465, 78, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((67, 1), 3) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((465, 1), 4) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((299, 1), 5) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 78, 128)\n",
      "(176, 78, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((176, 1), 6) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((734, 1), 7) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(734, 78, 128)\n",
      "(1000, 78, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lno7761\\AppData\\Local\\Continuum\\Miniconda3\\envs\\py35\\lib\\site-packages\\numpy\\core\\numeric.py:301: FutureWarning: in the future, full((1000, 1), 0) will return an array of dtype('int32')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def merge_datasets(pickle_files):    \n",
    "    \n",
    "  dataset_arr = np.empty(shape=(0,image_size_h,image_size_w))\n",
    "  dataset_label = np.array([])\n",
    "  fish_names=[]\n",
    "  for label, pickle_file in enumerate(pickle_files):\n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        fish_set = pickle.load(f)\n",
    "        print(fish_set.shape)\n",
    "        n_rows = fish_set.shape[0]\n",
    "        label_set=np.full(shape=(n_rows,1),fill_value=label)\n",
    "        dataset_arr=np.append(dataset_arr,fish_set,axis=0)\n",
    "        dataset_label=np.append(dataset_label,label_set)\n",
    "        fish_names.append(pickle_file)    \n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return dataset_arr, dataset_label, fish_names\n",
    "            \n",
    "            \n",
    "train_dataset, train_labels, fish_names_train = merge_datasets(train_datasets)\n",
    "test_dataset, test_labels, fish_names_test = merge_datasets(test_datasets)\n",
    "#test_dataset_flat=test_dataset.reshape(test_dataset.shape[0],78*128)\n",
    "#print('Training:', train_dataset.shape, train_labels.shape)\n",
    "#print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "#print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 856 1300 3351 ..., 2919 2886  669] TEST: [2867 1636 1020 1331 3471  909  952 1621 2032  180 3379 2289  684  859 2870\n",
      " 3392 3434 3457  972 1905  387 3174 2708 1539 3362 2746 2554 1447 2159 2022\n",
      " 3202 1371  537 3070  488 1241 1138 3198  139  388  123 2728 3700 1778   36\n",
      "  637  423 3199 2649 1152 1947 2977 3093 2448 1221 3492 3002  978 1757  323\n",
      "  274 1198 2097 3234 2771 2711  770  121 2740 2262 3282 3247 2446  498  510\n",
      " 1808  290 1094 2098 3360 1868 2978 3646  324 2134 1584 3042 2308 3558 1509\n",
      " 3652 1828 3079 2325  291 1641  872 3030 1640 2899 3107 1589 2568 2612  701\n",
      " 3241 2181 3427 3513 3691 1204 3353 1040 1162 2674 3048 3607 3259 1979 1272\n",
      "  774 1843 1675 1441 2104 3150 1643 3405 1563  756 1496 3519 1873 1776  868\n",
      " 3140   84 1382 2961 3254  730 1466 1383  600 1946 2323 2223  146 2654 1033\n",
      "  894 3220  763 3686  256 2193 3303 3515 1922 1278 3411 1782 1653 3058 1925\n",
      " 2146  556 2545 1791 1395 3018 1216 3277 1570  791 3243 1400  705 2965 1455\n",
      " 2044  559 1284 2504 2421  860 2950  455 2665 1510 3755  286  908  821 3007\n",
      " 1153  719 1879 1913 1316  888 3756 1155 3000 1749 2001  277 2454 2910 2663\n",
      " 2656  164 1134 2633 3026 2620 3407   73 2161 1761 2431 3659 3193 3025 2635\n",
      " 3363 3435 2306  967 3767 1790 2340 2252 2063 3504 3679   69 3188 2196 3114\n",
      " 2682 3746 2450  628  221 1555  931  130 2887 2914 2514  777  147 3166 2659\n",
      " 2613 3218 2153 1512 3597 3765 1993  525 1176  176 1407 3171  816  956 2074\n",
      " 2123  199  429  297   72 1066 3631 2464 1345 3537 2956 3775 1940  368 3324\n",
      " 2165   67  750 2210  812   25 1435   86 3657 2142 1352 3454 1418 2584 3626\n",
      " 2778 1171  925 3452 1037 3266 2008 1624 2033 2752 1720  803  207 1948  275\n",
      "  212 1206 3507 1068  151 1601 2248 3078   41 2765 2644 3172 1568 3735 1462\n",
      " 2451 1771 2352 1313 2120   24 1713 2240 3005 1141   23  778  739 1597 3661\n",
      " 1067 2487 1035 2617 1898  697 2006  551 2544 3210 1304 2168  168  514 1297\n",
      " 3348  949 1485  976 2236 2585 2205 1071 1252 3032 3117  136 2385 2129 2646\n",
      " 1219 1690 1773 1289 2061 2348 1869 1978  849 1119 2522 1434  807  547  166\n",
      " 2737 1346  149  189 3447  201  663 1053 2228  560 2050 2170 1488 1104 3227\n",
      "  469 3361  617 2745 2499 2858 3740  885 1420 1004 3040 2572 1585  383  954\n",
      " 2478 1444 3294 1381 2506  335 2140 3426 2494 3242 1834 1476 3648 2718 3497\n",
      " 3665  973 1329 2520 2960 1802 1536  341   93  910  450 1830 2215 3412 2681\n",
      "   94 1703 3413 3342 3154 3466 3153 1022 1201 3431 2534  555 1405 2485  232\n",
      " 1855 1207 1208 2503 1764 1861 2226 1215 1492 2178 2047   33 2376 1250 2958\n",
      "  782 1987 3429 3522 1849 3122  639 1365 1645 2651  340 3709 1840 3180  377\n",
      " 1077  765 2179  509 2288 2121 1542  203 2701  886  257 3148 2758 2404  623\n",
      " 2317 2857 1085   87  373 2802 3758 1082  444 3024 2593 1662  433 3310 3039\n",
      " 2109  606   43 3446 2007 1373 2046  843 2065 3097 2893   99 3716 1392  197\n",
      " 2187 1469  659 2139 3593  234   57 1673 3712 2705  835 2315  594 2706  296\n",
      " 3359 1312  398  710 1742 3512 1574  928  287 2364 3733 3470 1107 2291 3238\n",
      " 1293 1839 2733  607 1693 1143 1445  244 1815 1744  307 2736  919 2090 3017\n",
      " 1723 3233   28  633 3306 3722  889 2003  611  964  809  273  865 1472 2396\n",
      " 3442 2903 2254 2528 3705 2683   32  802 3162 3666 2102 3344 2301 2871 1060\n",
      "  543 2141   98 1836 2875 2372 2284 2671  562 3112 2365  696 1167 1172 1111\n",
      "  735   11 2630 2116 2851  714 2837 1338  111  314 1663  160   91 1007  749\n",
      "  480 3433  591 2162  767 1251 3608 1531 3186  174 1943  673 2801 3455 2355\n",
      " 2810 1980 3592  115 1807 3602 2069 3469  850 3374 1350 3350 2828 2020  797\n",
      "  804 1571  584  129 1106 2915 3045 3175 3404  508  544  734 1065 3717 1605\n",
      " 1569 1878 2996 3147 1043  779 3099 2622  882 1117 3561 2693 1983 1945 1578\n",
      " 2399  876 2729 1112  169 2983 3729 3170  448 2016 2551  128 2610 2281  806\n",
      "  727 2823  599 2767  209 3212  370  917 3491  430 1679  675  645  106 2299\n",
      " 2231 1801  755 2390 3187 2386 1726 1884  598  329 3226 2637 3596 2304 2294\n",
      " 2601 3418 1417 1483  216  845]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#train_dataset_flat=train_dataset.reshape(train_dataset.shape[0],78*128)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(train_dataset, train_labels):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_val = train_dataset[train_index,:,:], train_dataset[test_index,:,:]\n",
    "    y_train, y_val = train_labels[train_index], train_labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3021, 78, 128)\n",
      "(756, 78, 128)\n",
      "(1000, 78, 128)\n",
      "(3021,)\n",
      "(756,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(test_dataset.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'thumbnail_78_128_12Dec.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_datasets': X_train,\n",
    "    'train_labels': y_train,\n",
    "    'val_datasets': X_val,\n",
    "    'val_labels': y_val,\n",
    "    'test_datasets' : test_dataset,\n",
    "    'test_images' : test_images,\n",
    "    'fish_names_train' : fish_names_train,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
